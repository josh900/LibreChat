// Debug what the endpoints config actually looks like
console.log('ðŸ” Debugging Endpoints Configuration');

// Test 1: Check the raw endpoints config
fetch('/api/config')
  .then(res => res.json())
  .then(config => {
    console.log('ðŸ“‹ Raw config:', config);
    console.log('ðŸ” Endpoints in config:', config.endpoints);
  })
  .catch(err => console.error('âŒ Config fetch failed:', err));

// Test 2: Check what the endpoints query returns
setTimeout(() => {
  console.log('ðŸ” Checking endpoints query...');
  if (window.librechatDataProvider) {
    const { useGetEndpointsQuery } = window.librechatDataProvider.reactQuery;
    console.log('âœ… Data provider available');
    console.log('ðŸ” useGetEndpointsQuery:', useGetEndpointsQuery);
  } else {
    console.log('âŒ Data provider not available');
  }
}, 2000);

// Test 3: Manual endpoint config check
setTimeout(() => {
  console.log('ðŸ” Manual endpoints config check...');
  const endpointsConfig = {
    LiteLLM: {
      name: 'LiteLLM',
      apiKey: 'user_provided',
      baseURL: 'https://litellm.skoop.digital/v1',
      models: {
        fetch: true,
        default: ['gemini/gemini-2.0-flash-lite', 'openai/gpt-4o']
      }
    }
  };

  console.log('ðŸ“Š Manual endpoints config:', endpointsConfig);

  const userProvidedEndpoints = Object.entries(endpointsConfig)
    .filter(([_, config]) => {
      console.log('ðŸ” Checking endpoint:', _, 'config:', config);
      const isUserProvided = config?.apiKey === 'user_provided';
      const hasFetch = config?.models?.fetch;
      console.log('  - apiKey check:', config?.apiKey, '=== user_provided:', isUserProvided);
      console.log('  - fetch check:', config?.models?.fetch, '=== true:', hasFetch);
      return isUserProvided && hasFetch;
    })
    .map(([endpoint]) => endpoint);

  console.log('âœ… Filtered user-provided endpoints:', userProvidedEndpoints);
}, 1000);
